---
title: "Prepare genotypic data"
author: "Marnin Wolfe"
date: "2022-04-01"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: inline
---

0. Objectives
1. Any major issues setting up to login to server?
2. Plan: next Tuesday additional server time
3. Process map
4. vcf and bcftools manuals
5. Get started


```{r, setup, include=FALSE}
knitr::opts_chunk$set(eval=F)
```

## Remote access the server

### If on Cornell network

```{bash, eval=F}
ssh mw489@cbsumm21.biohpc.cornell.edu
```

### External to Cornell network

First, `ssh` the "login" server
```{bash}
ssh userid@cbsulogin2.biohpc.cornell.edu
```
Enter password

Now `ssh` from login to the reserved server
```{bash}
ssh userid@cbsumm21.biohpc.cornell.edu
```


## Create a directory on `/workdir/` for days work

Go to `/workdir/`

```{bash, eval=F}
cd /workdir/
```

Create a directory with _your_ username:

Example:
```{bash, eval=F}
mkdir mw489/;
```

Create a sub-directory for _this_ analysis.

Example:

```{bash, eval=F}
mkdir /workdir/mw489/GSexample2022
```

Navigate into your subdirectory on `/workdir/` (empty now).

Example:
```{bash, eval=F}
cd /workdir/youruserid/GSexample2022
```

Copy the `data/` and `output/` from _my_ `/workdir/mw489/GSexample2022/` location, into yours.

Example:
```{bash, eval=F}
cp -r /workdir/mw489/GSexample2022/data /workdir/youruserid/GSexample2022/
cp -r /workdir/mw489/GSexample2022/output /workdir/youruserid/GSexample2022/
```

Check that its there. Example:
```{bash, eval=F}
ls /workdir/youruserid/GSexample2022/data/
```

## Check the VCF

```{bash}
vcftools --vcf data/BreedBaseGenotypesDownload.vcf
cat data/BreedBaseGenotypesDownload.vcf | head -n50 | cut -c1-100
```

## Subset VCF

### Remove duplicates

```{bash}
bcftools query --list-samples data/BreedBaseGenotypesDownload.vcf
```

Manual solution
```{bash}
egrep "^#CHROM" data/BreedBaseGenotypesDownload.vcf | head -n1 > data/vcf_colnames.txt
```

```{r}
library(genomicMateSelectR)
vcf_sample_names<-readLines("data/vcf_colnames.txt") %>% 
     strsplit(.,"\t") %>% unlist() %>% 
     .[10:length(.)]
table(duplicated(vcf_sample_names))

```

```{r}
# create unique names for each VCF
unique_names_for_vcf<-tibble(vcfName=vcf_sample_names) %>% 
     # create an overall index to ensure I can recover the original column order
     mutate(vcfIndex=1:n()) %>% 
     # now for each vcfName create an sub-index, to distinguish among duplicates
     group_by(vcfName) %>% 
     # sub-index
     mutate(vcfNameIndex=1:n(),
            # For the first (or only) instance of each unique vcfName
            vcfName_Unique=ifelse(vcfNameIndex==1,
                                  # return the original name
                                  vcfName,
                                  # for all subsequent (duplicate) names, 
                                  #put a unique-ified name by pasting the sub-index
                                  paste0(vcfName,".",vcfNameIndex)))
# Write the "unique_names_for_vcf.txt" to disk
write.table(unique_names_for_vcf$vcfName_Unique,file = "data/unique_names_for_vcf.txt",
            row.names = F, col.names = F, quote = F)
# Create also a list containing only one instance of each unique name, the first instance 
subset_unique_names_for_vcf<-unique_names_for_vcf %>% 
     filter(vcfNameIndex==1) %$%
     vcfName_Unique
# Write that list to disk for subsetting the VCF downstream
write.table(subset_unique_names_for_vcf,file = "data/subset_unique_names_for_vcf.txt",
            row.names = F, col.names = F, quote = F)
```

Show what this did:
```{r}
unique_names_for_vcf%>%filter(vcfName=="TMS18F1294P0011")
```

```{bash}
# replace sample names in original VCF with unique ones (creates a new VCF)
bcftools reheader --samples data/unique_names_for_vcf.txt data/BreedBaseGenotypesDownload.vcf > data/BreedBaseGenotypesDownload_1.vcf; 
# overwrite the original VCF with the new  that has unique names
mv data/BreedBaseGenotypesDownload_1.vcf data/BreedBaseGenotypesDownload.vcf;
# check that the names are now unique by printing sample list
bcftools query --list-samples data/BreedBaseGenotypesDownload.vcf
```

```{bash}
vcftools --vcf data/BreedBaseGenotypesDownload.vcf --keep data/subset_unique_names_for_vcf.txt --recode --stdout | bgzip -c > data/BreedBaseGenotypes_subset.vcf.gz
```

```{bash}
vcftools --gzvcf data/BreedBaseGenotypes_subset.vcf.gz
```

### Check genotype-to-phenotype matches

```{r}
phenos<-readRDS(here::here("output","phenotypes_cleaned.rds"))

# vector of the unique germplasmName in the field trial data
germplasm_with_phenos<-unique(phenos$germplasmName)
length(germplasm_with_phenos)
```

```{r}
subset_unique_names_for_vcf<-read.table(file = "data/subset_unique_names_for_vcf.txt", 
                                        stringsAsFactors = F)$V1

table(germplasm_with_phenos %in% subset_unique_names_for_vcf)
```

```{r}
# geno and pheno
subset_unique_names_for_vcf[subset_unique_names_for_vcf %in% germplasm_with_phenos]
# pheno not geno
germplasm_with_phenos[!germplasm_with_phenos %in% subset_unique_names_for_vcf]
# geno not pheno
subset_unique_names_for_vcf[!subset_unique_names_for_vcf %in% germplasm_with_phenos]
```

### Subset SNPs (for tutorial purposes)

```{bash, eval=F}
# write the positions list
# first two columns (chrom. and position) of the VCF 
# ignoring the header rows
cat data/BreedBaseGenotypesDownload.vcf | grep -v "^#" | cut -f1-2 > data/BreedBaseGenotypesDownload.positions
```

Read into R, sample 4000 at random

```{r}
library(genomicMateSelectR)
set.seed(1234)
read.table(here::here("data","BreedBaseGenotypesDownload.positions"), 
           header = F, stringsAsFactors = F) %>% 
     dplyr::slice_sample(n=4000) %>% 
     arrange(V1,V2) %>% 
     write.table(.,file = "data/BreedBaseGenotypes_subset.positions",
                 row.names = F, col.names = F, quote = F)

```

```{bash}
vcftools --vcf data/BreedBaseGenotypesDownload.vcf \
--keep data/subset_unique_names_for_vcf.txt \
--positions data/BreedBaseGenotypes_subset.positions \
--recode --stdout | bgzip -c > data/BreedBaseGenotypes_subset.vcf.gz
```

```{bash}
vcftools --gzvcf data/BreedBaseGenotypes_subset.vcf.gz
```

## Haplotype matrix from VCF

Extract haplotypes from VCF with `bcftools convert --hapsample`

```{bash, eval=F}
bcftools convert --hapsample data/BreedBaseGenotypes_subset data/BreedBaseGenotypes_subset.vcf.gz
```

Read haps to R and format them.

```{r}
library(genomicMateSelectR)
library(data.table)
vcfName<-"BreedBaseGenotypes_subset"
haps<-fread(paste0("data/",vcfName,".hap.gz"),
            stringsAsFactors = F,header = F) %>% 
  as.data.frame
sampleids<-fread(paste0("data/",vcfName,".samples"),
                 stringsAsFactors = F,header = F,skip = 2) %>% 
  as.data.frame
```

Add sample ID's.

```{r}
hapids<-sampleids %>% 
     select(V1,V2) %>% 
     mutate(SampleIndex=1:nrow(.)) %>% 
     rename(HapA=V1,HapB=V2) %>% 
     pivot_longer(cols=c(HapA,HapB),
                  names_to = "Haplo",values_to = "SampleID") %>% 
     mutate(HapID=paste0(SampleID,"_",Haplo)) %>% 
     arrange(SampleIndex)
colnames(haps)<-c("Chr","HAP_ID","Pos","REF","ALT",hapids$HapID)
dim(haps)
```

Format, transpose, convert to matrix.

```{r}
haps %<>% 
     mutate(HAP_ID=gsub(":","_",HAP_ID)) %>% 
     column_to_rownames(var = "HAP_ID") %>% 
     select(-Chr,-Pos,-REF,-ALT) %>% 
     t(.) %>% 
     as.matrix(.)
dim(haps)
```

## Dosage matrix from haps
```{r}
dosages<-haps %>%
     as.data.frame(.) %>% 
     rownames_to_column(var = "GID") %>% 
     separate(GID,c("SampleID","Haplo"),"_Hap",remove = T) %>% 
     select(-Haplo) %>% 
     group_by(SampleID) %>% 
     summarise(across(everything(),~sum(.))) %>% 
     ungroup() %>% 
     column_to_rownames(var = "SampleID") %>% 
     as.matrix %>% 
     # preserve same order as in haps
     .[sampleids$V1,]
dim(dosages)
# [1]  963 4000

dosages[1:5,1:5]
```

```{r}
haps[1:10,1:5]
```

## Variant filters


```{r}
# use function built into genomicMateSelectR
dosages<-maf_filter(dosages,thresh = 0.01)
dim(dosages)
# subset haps to match
haps<-haps[,colnames(dosages)]
```

### Save dosages and haps

```{r}
saveRDS(dosages,file=here::here("data","dosages.rds"))
saveRDS(haps,file=here::here("data","haplotypes.rds"))
```

## Genomic Relationship Matrices (GRMs)

```{r}
A<-kinship(dosages,type="add")
D<-kinship(dosages,type="domGenotypic")
saveRDS(A,file=here::here("output","kinship_add.rds"))
saveRDS(D,file=here::here("output","kinship_dom.rds"))
```

## Recombination Frequency Matrix

### Source a genetic map
```{r}
genmap<-read.table("https://cassavabase.org/ftp/marnin_datasets/NGC_BigData/CassavaGeneticMap/cassava_cM_pred.v6.allchr.txt",
                   header = F, sep=';', stringsAsFactors = F) %>% 
     rename(SNP_ID=V1,Pos=V2,cM=V3) %>% 
  as_tibble
genmap %>% dim
```

120K positions.

```{r}
genmap %>% head
```

```{r}
snps_genmap<-tibble(DoseSNP_ID=colnames(dosages)) %>% 
     separate(DoseSNP_ID,c("Chr","Pos","Ref","Alt"),remove = F) %>% 
     mutate(SNP_ID=paste0("S",Chr,"_",Pos)) %>% 
     full_join(genmap %>% 
                    separate(SNP_ID,c("Chr","POS"),"_",remove = F) %>% 
                    select(-POS) %>% 
                    mutate(Chr=gsub("S","",Chr)) %>% 
                    mutate(across(everything(),as.character)))
```

### Interpolate genetic map

```{r}
interpolate_genmap<-function(data){
  # for each chromosome map
  # find and _decrements_ in the genetic map distance
  # fix them to the cumulative max to force map to be only increasing
  # fit a spline for each chromosome
  # Use it to predict values for positions not previously on the map
  # fix them AGAIN (in case) to the cumulative max, forcing map to only increase
  data_forspline<-data %>% 
    filter(!is.na(cM)) %>% 
    mutate(cumMax=cummax(cM),
           cumIncrement=cM-cumMax) %>% 
    filter(cumIncrement>=0) %>% 
    select(-cumMax,-cumIncrement)
  
  spline<-data_forspline %$% smooth.spline(x=Pos,y=cM,spar = 0.75)
  
  splinemap<-predict(spline,x = data$Pos) %>% 
    as_tibble(.) %>% 
    rename(Pos=x,cM=y) %>% 
    mutate(cumMax=cummax(cM),
           cumIncrement=cM-cumMax) %>% 
    mutate(cM=cumMax) %>% 
    select(-cumMax,-cumIncrement)
  
  return(splinemap) 
}
```

```{r}
splined_snps_genmap<-snps_genmap %>% 
  select(-cM) %>% 
  mutate(Pos=as.numeric(Pos)) %>% 
  left_join(snps_genmap %>% 
              mutate(across(c(Pos,cM),as.numeric)) %>% 
              arrange(Chr,Pos) %>% 
              nest(-Chr) %>% 
              mutate(data=map(data,interpolate_genmap)) %>% 
              unnest(data)) %>% 
  distinct
```

```{r}
all(colnames(dosages) %in% splined_snps_genmap$DoseSNP_ID)
```

Save the interpolated map, just for the marker loci to-be-used downstream.

```{r}
splined_snps_genmap %>% 
     filter(DoseSNP_ID %in% colnames(dosages)) %>% 
     saveRDS(.,file=here::here("output","interpolated_genmap.rds"))
```

### Recomb. freq. matrix

```{r}
genmap<-readRDS(file=here::here("output","interpolated_genmap.rds"))
m<-genmap$cM;
names(m)<-genmap$DoseSNP_ID
recombFreqMat<-1-(2*genmap2recombfreq(m,nChr = 18))
saveRDS(recombFreqMat,file=here::here("output","recombFreqMat_1minus2c.rds"))
```

## Copy files back to your computer

Option 1) FileZilla

Option 2) Command line:

On my local computer, on your command line.

Navigate to the project directory containing the `data/` and `output/` directories.

```{bash, eval=F}
scp mw489@cbsumm21.biohpc.cornell.edu:/workdir/youruserid/yourprojectname/data/BreedBaseGenotypes_subset.vcf.gz data/
scp mw489@cbsumm21.biohpc.cornell.edu:/workdir/youruserid/yourprojectname/data/dosages.rds data/
scp mw489@cbsumm21.biohpc.cornell.edu:/workdir/youruserid/yourprojectname/data/haplotypes.rds data/
scp mw489@cbsumm21.biohpc.cornell.edu:/workdir/youruserid/yourprojectname/output/kinship_add.rds output/
scp mw489@cbsumm21.biohpc.cornell.edu:/workdir/youruserid/yourprojectname/output/kinship_dom.rds output/
scp mw489@cbsumm21.biohpc.cornell.edu:/workdir/youruserid/yourprojectname/output/recombFreqMat_1minus2c.rds output/
scp mw489@cbsumm21.biohpc.cornell.edu:/workdir/youruserid/yourprojectname/output/interpolated_genmap.rds output/
```

Option 3) I'll provide Google Drive links to download the big files, and post the small ones to my GitHub repo